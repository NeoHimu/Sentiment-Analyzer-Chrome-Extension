{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a Transformer-based architecture on the IMDB Movie Reviews dataset for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "tqdm\n",
    "torch==1.1.0\n",
    "pytorch_transformers\n",
    "pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_html(raw: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "def read_imdb(data_dir, max_lengths={\"train\": None, \"test\": None}):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"imdb5k_{t}.csv\"))\n",
    "        if max_lengths.get(t) is not None:\n",
    "            df = df.sample(n=max_lengths.get(t))\n",
    "            df[TEXT_COL] = df[TEXT_COL].apply(lambda t: clean_html(t))\n",
    "        datasets[t] = df\n",
    "    return datasets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data, 5000-5000 each\n",
    "import os\n",
    "IMDB_DIR = \"\"\n",
    "LABEL_COL = \"label\"\n",
    "datasets = read_imdb(IMDB_DIR, max_lengths={\"train\": None, \"test\": None})\n",
    "\n",
    "# list of labels\n",
    "labels = list(set(datasets[\"train\"][LABEL_COL].tolist()))\n",
    "\n",
    "# labels to integers mapping\n",
    "label2int = {label: i for i, label in enumerate(labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "      <td>Bah. Another tired, desultory reworking of an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>Twist endings can be really cool in a movie. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>Every time I watch this movie I am more impres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>I absolutely LOVED this film! I do not at all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>Though the plot elements to \"The Eighth Day\" s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   neg  Bah. Another tired, desultory reworking of an ...\n",
       "1   neg  Twist endings can be really cool in a movie. I...\n",
       "2   pos  Every time I watch this movie I am more impres...\n",
       "3   pos  I absolutely LOVED this film! I do not at all ...\n",
       "4   pos  Though the plot elements to \"The Eighth Day\" s..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\"\"\"utils.py: Utility code for Transformer fine-tuning on the IMDB Movie Reviews dataset.\"\"\"\n",
    "\n",
    "__author__ = \"Oliver Atanaszov\"\n",
    "__email__ = \"oliver.atanaszov@gmail.com\"\n",
    "__github__ = \"https://github.com/ben0it8\"\n",
    "__copyright__ = \"Copyright 2019, Planet Earth\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from itertools import repeat\n",
    "from datetime import timedelta\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, cached_path, AdamW\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "\n",
    "logger = logging.getLogger(\"utils.py\")\n",
    "\n",
    "num_cores = cpu_count()\n",
    "\n",
    "# text and label column names\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "FineTuningConfig = namedtuple(\n",
    "    'FineTuningConfig',\n",
    "    field_names=\n",
    "    \"num_classes, dropout, init_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "    \"n_warmup, valid_pct, gradient_acc_steps, device, log_dir\")\n",
    "\n",
    "class TextProcessor:\n",
    "\n",
    "    # special tokens for classification and padding\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "\n",
    "    def __init__(self, tokenizer, label2id: dict,\n",
    "                 num_max_positions: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions\n",
    "\n",
    "    def process_example(self, example: Tuple[str, str]):\n",
    "        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions - 1]\n",
    "            ids = self.tokenizer.convert_tokens_to_ids(tokens) + [\n",
    "                self.tokenizer.vocab[self.CLS]\n",
    "            ]\n",
    "        # pad if too short\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]\n",
    "                   ] * (self.num_max_positions - len(tokens) - 1)\n",
    "            ids = self.tokenizer.convert_tokens_to_ids(tokens) + [\n",
    "                self.tokenizer.vocab[self.CLS]\n",
    "            ] + pad\n",
    "\n",
    "        return ids, self.label2id[label]\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings,\n",
    "                 num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(\n",
    "        ), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(\n",
    "                nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(\n",
    "                nn.Sequential(nn.Linear(embed_dim, hidden_dim), nn.ReLU(),\n",
    "                              nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)),\n",
    "                                   -float('Inf'),\n",
    "                                   device=h.device,\n",
    "                                   dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(\n",
    "                self.layer_norms_1, self.attentions, self.layer_norms_2,\n",
    "                self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h,\n",
    "                             h,\n",
    "                             h,\n",
    "                             attn_mask=attn_mask,\n",
    "                             need_weights=False,\n",
    "                             key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim,\n",
    "                                       config.hidden_dim,\n",
    "                                       config.num_embeddings,\n",
    "                                       config.num_max_positions,\n",
    "                                       config.num_heads,\n",
    "                                       config.num_layers,\n",
    "                                       fine_tuning_config.dropout,\n",
    "                                       causal=not config.mlm)\n",
    "\n",
    "        self.classification_head = nn.Linear(config.embed_dim,\n",
    "                                             fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module,\n",
    "                      (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states *\n",
    "                             clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)),\n",
    "                            clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits\n",
    "\n",
    "\n",
    "def getenv_cast(var: str, cast=None):\n",
    "    value = os.getenv(var)\n",
    "    if value is not None and cast is not None:\n",
    "        value = cast(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def freeze_body(model):\n",
    "    for idx, (_, child) in enumerate(model.named_children()):\n",
    "        if idx == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            freeze_body(child)\n",
    "\n",
    "\n",
    "def timeit(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        delta = timedelta(seconds=round(time() - ts, 1))\n",
    "        print(f\"Elapsed time for {f.__name__}: {str(delta):0>8}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def download_url(url: str,\n",
    "                 dest: str,\n",
    "                 overwrite: bool = True,\n",
    "                 show_progress=True,\n",
    "                 chunk_size=1024 * 1024,\n",
    "                 timeout=4,\n",
    "                 retries=5) -> None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    dest = os.path.join(dest, os.path.basename(url))\n",
    "    if os.path.exists(dest) and not overwrite:\n",
    "        logger.warning(f\"File {dest} already exists!\")\n",
    "        return dest\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://', requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try:\n",
    "        file_size = int(u.headers[\"Content-Length\"])\n",
    "    except:\n",
    "        show_progress = False\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress:\n",
    "            pbar = tqdm(range(file_size),\n",
    "                        leave=True,\n",
    "                        desc=f\"Downloading {os.path.basename(url)}\")\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.warning(f\"Download failed after {retries} retries.\")\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "        finally:\n",
    "            return dest\n",
    "\n",
    "\n",
    "@timeit\n",
    "def untar(file_path, dest: str):\n",
    "    \"Untar `file_path` to `dest`\"\n",
    "    logger.info(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))\n",
    "\n",
    "\n",
    "def clean_html(raw: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def read_imdb(data_dir, max_lengths={\"train\": None, \"test\": None}):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"imdb5k_{t}.csv\"))\n",
    "        maxlen = max_lengths.get(t)\n",
    "        if maxlen is not None and maxlen <= len(df):\n",
    "            df = df.sample(n=maxlen)\n",
    "        df[TEXT_COL] = df[TEXT_COL].apply(lambda t: clean_html(t))\n",
    "        datasets[t] = df\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def process_row(processor, row):\n",
    "    return processor.process_example((row[1][LABEL_COL], row[1][TEXT_COL]))\n",
    "\n",
    "\n",
    "def create_dataloader(df: pd.DataFrame,\n",
    "                      processor: TextProcessor,\n",
    "                      batch_size: int = 32,\n",
    "                      shuffle: bool = False,\n",
    "                      valid_pct: float = None,\n",
    "                      text_col: str = \"text\",\n",
    "                      label_col: str = \"label\"):\n",
    "    \"Process rows in `df` with `processor` and return a  DataLoader\"\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "        result = list(\n",
    "            tqdm(executor.map(process_row,\n",
    "                              repeat(processor),\n",
    "                              df.iterrows(),\n",
    "                              chunksize=len(df) // 10),\n",
    "                 desc=f\"Processing {len(df)} examples on {num_cores} cores\",\n",
    "                 total=len(df)))\n",
    "\n",
    "    features = [r[0] for r in result]\n",
    "    labels = [r[1] for r in result]\n",
    "\n",
    "    dataset = TensorDataset(torch.tensor(features, dtype=torch.long),\n",
    "                            torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "    if valid_pct is not None:\n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset,\n",
    "                                                    [valid_size, train_size])\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=0,\n",
    "                             shuffle=shuffle,\n",
    "                             pin_memory=torch.cuda.is_available())\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, int2label, device=None, input=\"test\"):\n",
    "    \"predict `input` with `model`\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    tok = tokenizer.tokenize(input)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tok) + [tokenizer.vocab['[CLS]']]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor.transpose(0, 1).contiguous()  # [S, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask=(tensor_in == tokenizer.vocab['[CLS]']),\n",
    "                   padding_mask=(tensor == tokenizer.vocab['[PAD]']))\n",
    "    val, _ = torch.max(logits, 0)\n",
    "    val = F.softmax(val, dim=0).detach().cpu().numpy()\n",
    "    return {\n",
    "        int2label[val.argmax()]: val.max(),\n",
    "        int2label[val.argmin()]: val.min()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottle webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bottle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_bert:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "INFO:pytorch_transformers.modeling_xlnet:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at C:\\Users\\Himanshu Singh\\.cache\\torch\\pytorch_transformers\\5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:app.py:Web app for movie sentiment analysis started. Inference device: cpu\n",
      "Bottle v0.12.17 server starting up (using WSGIRefServer())...\n",
      "Listening on http://127.0.0.1:5000/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'neg', 'neg', 'pos', 'pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2019 02:14:57] \"POST /social_media_posts HTTP/1.1\" 200 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'neg', 'neg', 'pos', 'pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2019 02:15:56] \"POST /social_media_posts HTTP/1.1\" 200 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2019 02:16:23] \"POST /social_media_posts HTTP/1.1\" 200 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'neg', 'neg', 'pos', 'pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2019 02:20:42] \"POST /social_media_posts HTTP/1.1\" 200 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2019 02:20:54] \"POST /social_media_posts HTTP/1.1\" 200 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2019 18:09:34] \"POST /social_media_posts HTTP/1.1\" 200 890\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python3\n",
    "from bottle import Bottle, run, request, response\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from json import dumps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from utils import TransformerWithClfHead, FineTuningConfig, predict\n",
    "\n",
    "logger = logging.getLogger(\"app.py\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased',\n",
    "                                          do_lower_case=False)\n",
    "\n",
    "int2label = {i:label for label,i in label2int.items()}\n",
    "\n",
    "metadata = torch.load(\"./logs/metadata.bin\")\n",
    "state_dict = torch.load(\"./logs/model_weights.pth\", map_location=device)\n",
    "\n",
    "model = TransformerWithClfHead(metadata[\"config\"], metadata[\"config_ft\"])\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "predict(model, tokenizer, int2label, device=None, input=\"This movie is poorly directed\")\n",
    "app = Bottle()\n",
    "\n",
    "#this is very important for other application to get access of the resource of the server\n",
    "def allow_cors(func):\n",
    "    \"\"\" this is a decorator which enable CORS for specified endpoint \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*' # * in case you want to be accessed via any website\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@app.route(\"/inference\")\n",
    "def inference():\n",
    "    return '''\n",
    "        <form action=\"/inference\" method=\"post\">\n",
    "            Text: <input name=\"text\" type=\"text\" />\n",
    "            <input value=\"Inference\" type=\"submit\" />\n",
    "        </form>\n",
    "        '''\n",
    "\n",
    "@app.route(\"/inference\", method='POST')\n",
    "def do_inference():\n",
    "    text = request.forms.get(\"text\")\n",
    "    print(f\"text: {text}\")\n",
    "    output = predict(model,\n",
    "                     tokenizer,\n",
    "                     metadata['int2label'],\n",
    "                     device=device,\n",
    "                     input=text)\n",
    "    print(output)\n",
    "    response.content_type = \"application/json\"\n",
    "    return dumps({k: str(v) for k, v in output.items()}, indent=4)\n",
    "\n",
    "\n",
    "import re, json\n",
    "\n",
    "namepattern = re.compile(r'^[a-zA-Z\\d]{1,64}$')\n",
    "\n",
    "@app.route(\"/social_media_posts\", method='POST')\n",
    "@allow_cors #this is very important\n",
    "def creation_handler():\n",
    "    '''Handles sentiment analysis for social media posts'''\n",
    "    #return dumps({\"message\": \"hello\"})\n",
    "    # parse input data\n",
    "    try:\n",
    "        social_media_posts = request.json.get('social_media_posts')\n",
    "    except:\n",
    "        raise ValueError\n",
    "\n",
    "    if social_media_posts is None:\n",
    "        raise ValueError\n",
    "\n",
    "    # extract and validate social_media_posts\n",
    "#     print(request)\n",
    "#     print(data.get('social_media_posts'))\n",
    "#     try:\n",
    "#         if namepattern.match(data.get('social_media_posts')) is None:\n",
    "#             raise ValueError\n",
    "#         social_media_posts = data.get('social_media_posts')\n",
    "#     except (TypeError, KeyError):\n",
    "#         raise ValueError\n",
    "    result=[]\n",
    "    for tweet in social_media_posts:\n",
    "        p = predict(model, tokenizer, int2label, device=None, input=tweet)\n",
    "        if(p[\"pos\"]>p[\"neg\"]):\n",
    "            result.append(\"pos\")\n",
    "        else:\n",
    "            result.append(\"neg\")\n",
    "    # return 200 Success\n",
    "    response.headers['Content-Type'] = 'application/json'\n",
    "    print(str(result))\n",
    "    return json.dumps({'social_media_posts': json.dumps(result)})\n",
    "\n",
    "\n",
    "logger.info(f\"Web app for movie sentiment analysis started. Inference device: {device}\")\n",
    "run(app, host=\"127.0.0.1\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
